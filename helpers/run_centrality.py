# Run PageRank and HITS in parallel for all the samples
# generated by the sampler type given in input
import sys
sys.path.insert(1, '../')
from helpers.config import n_iter, damping_factor, solver, tol, folder
from helpers.config import met_path, max_workers, sample_path
from helpers.centrality import parallel_hits, map_node_to_type, parallel_pagerank
from helpers.io import read_as_directed_hyperedge_list, read_as_edge_list, get_extended_edge_list_from_hypergraph
import zipfile
from copy import deepcopy
from collections import defaultdict
from tqdm.contrib.concurrent import process_map
from sknetwork.ranking import PageRank, HITS
from sknetwork.data import parse


if __name__ == '__main__':

    root = sys.argv[1]
    typ = sys.argv[2]
    data_path = folder + root
    print(root, typ)

    # run HITS observed hypergraph
    vmap2 = defaultdict(int)
    with open(f'{data_path}.tsv') as in_f:
        inv_vmap, edges = read_as_edge_list(in_f, vmap2)
    adj_mat = parse.from_edge_list(edges,
                                   reindex=False,
                                   matrix_only=True,
                                   directed=True,
                                   bipartite=False,
                                   weighted=False)
    hits = HITS()
    hits.fit(adj_mat)
    scores_h, scores_a = hits.scores_row_, hits.scores_col_
    with open(met_path + f"hits_{root}.tsv", 'w') as out_f:
        out_f.write('Node Id\tHub Score\tAuthority Score\tType\n')
        for idx, hs in enumerate(scores_h):
            ntyp = map_node_to_type(inv_vmap[idx])
            out_f.write(f'{idx}\t{hs}\t{scores_a[idx]}\t{ntyp}\n')

    # run HITS samples
    vmap3 = {k: v for k, v in vmap2.items() if k[0] == "L"}

    file_path = f'{sample_path}/{typ}/{root}.zip'
    out_f = open(met_path + f"hits_{root}_samples_{typ}.tsv", 'w')
    out_f.write(
        'Node Id\tSampler\tSample Id\tHub Score\tAuthority Score\tType\n')

    z = zipfile.ZipFile(file_path, "r")
    zinfo = z.namelist()
    inputs = []
    for file_name in zinfo:
        if file_name.startswith(".") or file_name.startswith("__MACOSX"):
            continue
        if not file_name.endswith('.tsv'):
            continue
        inputs.append([file_path, file_name, deepcopy(vmap3), typ])
    outputs = process_map(parallel_hits, inputs, max_workers=max_workers)
    for out in outputs:
        for row in out:
            out_f.write('\t'.join(row) + '\n')
    out_f.close()
    print('hits scores computed.')

    # run PageRank observed hypergraph
    vmap = defaultdict(int)
    with open(f'{data_path}.tsv') as in_f:
        heads, tails = read_as_directed_hyperedge_list(in_f, vmap)

    ext_edge_list = get_extended_edge_list_from_hypergraph(heads,
                                                           tails,
                                                           with_int_edges=False)
    A = parse.from_edge_list(ext_edge_list,
                             reindex=False,
                             matrix_only=True,
                             directed=True,
                             weighted=True)
    pagerank = PageRank(n_iter=n_iter, damping_factor=damping_factor,
                        solver=solver, tol=tol)
    scores_pr = pagerank.fit_predict(A)

    with open(met_path + f"pagerank_{root}.tsv", 'w') as out_f:
        out_f.write('Node Id\tPageRank\n')
        for idx, pr in enumerate(scores_pr):
            out_f.write(f'{idx}\t{pr}\n')

    # run PageRank samples
    out_f = open(met_path + f"pagerank_{root}_samples_{typ}.tsv", 'w')
    out_f.write('Node Id\tSampler\tSample Id\tPageRank\n')
    inputs = []
    for file_name in zinfo:
        if file_name.startswith(".") or file_name.startswith("__MACOSX"):
            continue
        if not file_name.endswith('.tsv'):
            continue
        inputs.append([file_path, file_name, vmap, typ])
    outputs = process_map(parallel_pagerank,
                          inputs,
                          max_workers=max_workers)
    for out in outputs:
        for row in out:
            out_f.write('\t'.join(row) + '\n')
    out_f.close()
    print('PageRank scores computed.')
